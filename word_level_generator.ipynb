{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'songs.txt'\n",
    "with open(data_dir, \"r\", encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "total words:211681\na batch of data:['Does', \"n't\", 'take', 'much', 'to', 'make', 'me', 'happy', 'And', 'make']\n"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(data)\n",
    "print('total words:', len(tokens))\n",
    "print('a batch of data:', tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    vocab_to_int = {word: i for i, word in enumerate(sorted_vocab)}\n",
    "    int_to_vocab = {word: i for i, word in vocab_to_int.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "training on GPU\n"
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "print('training on', ' GPU' if train_on_gpu else ' CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def create_batches(words, sequence_length, batch_size):\n",
    "\n",
    "    n_batches = len(words)//batch_size\n",
    "    words = words[:n_batches*batch_size]\n",
    "\n",
    "    features, targets = [], []\n",
    "    for idx in range(0, (len(words) - sequence_length) ):\n",
    "        features.append(words[idx : idx + sequence_length])\n",
    "        targets.append(words[idx + sequence_length])   \n",
    "        \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(features)), torch.from_numpy(np.asarray(targets)))\n",
    "    data_loader = torch.utils.data.DataLoader(data, shuffle=False , batch_size = batch_size)\n",
    "\n",
    "    # return a dataloader\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    # perform backpropagation and optimization\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    target = torch.tensor(target).to(torch.int64)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, save_every=0):\n",
    "    \n",
    "    batch_losses = []\n",
    "    total_time = 0\n",
    "\n",
    "    rnn.train()\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "\n",
    "        start.record()\n",
    "\n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            import sys\n",
    "            progress = batch_i * 100/n_batches\n",
    "            print('\\r' + 'Epoch: {:>4}/{:<4}  Loss: {:7}, epoch progress: {:2}%'.format(\n",
    "                epoch_i, n_epochs, str(np.average(batch_losses))[:14], str(progress)[:4]), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            batch_losses = []\n",
    "\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        time = start.elapsed_time(end)\n",
    "        print('\\r')\n",
    "        print('epoch finished in: '+ str(time/60000)[:5], ' minutes')\n",
    "        total_time += time    \n",
    "\n",
    "        if (epoch_i == n_epochs):\n",
    "            print('\\r')\n",
    "            print('training finished in: '+ str(total_time/60000)[:8], ' minutes')\n",
    "\n",
    "        if save_every != 0 and epoch_i % save_every == 0 and epoch_i != n_epochs:\n",
    "            print('saving model...')\n",
    "            torch.save(rnn.state_dict(), 'saved_models/trained_rnn_' + str(epoch_i) + '.pt')\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "a sample of text:[1090, 9, 104, 125, 5]\n"
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "batch_size = 64\n",
    "\n",
    "int_text = [vocab_to_int[word] for word in tokens]\n",
    "print('a sample of text:', int_text[:5])\n",
    "\n",
    "train_loader = create_batches(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim = 512\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training for 20 epoch(s)...\nEpoch:    1/20    Loss: 5.799723625183, epoch progress: 100.%\nepoch finished in:1.899 minutes\nEpoch:    2/20    Loss: 5.585914134979, epoch progress: 100.%\nepoch finished in:1.889 minutes\nEpoch:    3/20    Loss: 5.286500930786, epoch progress: 100.%\nepoch finished in:1.898 minutes\nEpoch:    4/20    Loss: 5.080548286437, epoch progress: 100.%\nepoch finished in:1.883 minutes\nEpoch:    5/20    Loss: 4.889713764190, epoch progress: 100.%\nepoch finished in:1.882 minutes\nsaving model...\nEpoch:    6/20    Loss: 4.686353683471, epoch progress: 100.%\nepoch finished in:2.755 minutes\nEpoch:    7/20    Loss: 4.731459140777, epoch progress: 100.%\nepoch finished in:1.893 minutes\nEpoch:    8/20    Loss: 4.726864337921, epoch progress: 100.%\nepoch finished in:2.684 minutes\nEpoch:    9/20    Loss: 4.481072425842, epoch progress: 100.%\nepoch finished in:1.931 minutes\nEpoch:   10/20    Loss: 4.199986457824, epoch progress: 100.%\nepoch finished in:1.867 minutes\nsaving model...\nEpoch:   11/20    Loss: 4.321308612823, epoch progress: 100.%\nepoch finished in:1.853 minutes\nEpoch:   12/20    Loss: 4.163632392883, epoch progress: 100.%\nepoch finished in:1.927 minutes\nEpoch:   13/20    Loss: 4.258933067321, epoch progress: 100.%\nepoch finished in:2.578 minutes\nEpoch:   14/20    Loss: 3.981896162033, epoch progress: 100.%\nepoch finished in:2.609 minutes\nEpoch:   15/20    Loss: 4.017460823059, epoch progress: 100.%\nepoch finished in:1.909 minutes\nsaving model...\nEpoch:   16/20    Loss: 4.034354686737, epoch progress: 100.%\nepoch finished in:2.524 minutes\nEpoch:   17/20    Loss: 3.914153814315, epoch progress: 100.%\nepoch finished in:1.851 minutes\nEpoch:   18/20    Loss: 3.904424428939, epoch progress: 100.%\nepoch finished in:2.509 minutes\nEpoch:   19/20    Loss: 3.827639102935, epoch progress: 100.%\nepoch finished in:1.843 minutes\nEpoch:   20/20    Loss: 3.835208415985, epoch progress: 100.%\nepoch finished in:1.864 minutes\n\ntraining finished in:42.05 minutes\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import models\n",
    "# create model and move to gpu if available\n",
    "rnn = models.RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "try:\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, save_every=5)\n",
    "    # saving the trained model\n",
    "    torch.save(trained_rnn, 'saved_models/final_trained_rnn.pt')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('\\ntraining stopped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'saved_models/final_trained_rnn.pt'\n",
    "picked_rnn = torch.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        if train_on_gpu:\n",
    "            current_seq = current_seq.cpu()\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "love , when love is all you need to love me Bright lips of the rain of mine Well , you 've collected you cry And I 'm amazed at the way you 're near Words of my life and mine you 'll be a part of mine . And if you 're next , I 've been thinking about you ) I 'm all alone ( I 'm gon na stick like glue Stick because I 'm stuck on you Hide in the kitchen , hide around his steed covered to wail That they 've swung us since we 've\n"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "token_dict  = {}\n",
    "# run the cell multiple times to get different results!\n",
    "gen_length = 100 # modify the length to your preference\n",
    "prime_word = 'love' # name for starting the script\n",
    "\n",
    "token_dict = {\".\":\"Period\", \n",
    "            \",\": \"Comma\",\n",
    "            \"\\\"\":\"Quotation_Mark\",\n",
    "            \";\": \"Semicolon\",\n",
    "            \"!\":\"Exclamation_mark\",\n",
    "            \"?\":\"Question_mark\",\n",
    "            \"(\":\"Left_Parentheses\",\n",
    "            \")\":\"Right_Parentheses\",\n",
    "            \"-\":\"Dash\",\n",
    "            \"\\n\":\"Return\"}\n",
    "\n",
    "generated_script = generate(picked_rnn, vocab_to_int[prime_word], int_to_vocab, token_dict, vocab_to_int['love'], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "song = open(\"generated_song_1.txt\", \"w\")\n",
    "song.write(generated_script)\n",
    "song.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbasecondabbbd6f6fc33d437fafd772992c71399d",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}